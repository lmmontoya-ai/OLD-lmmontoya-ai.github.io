---
title: "Attention Head Attribution in Large Language Models"
date: "2024-11-15"
area: "Interpretability"
collaborators: ["Jane Doe", "John Smith"]
organization: "AI Safety Lab"
summary: "Developed novel metrics for quantifying attention head contributions to factual recall."
tags: ["attention", "attribution", "LLMs", "factuality"]
github_link: "https://github.com/username/project"
paper_link: "https://arxiv.org/abs/..."
hero_image: "/images/projects/attention-attribution.svg"
featured: true
---

# Methodology

We used [Integrated Gradients](https://arxiv.org/abs/1703.01365) to quantify...

<Figure
  src="/images/projects/attention-attribution.svg"
  alt="Attention Attribution Diagram"
/>
