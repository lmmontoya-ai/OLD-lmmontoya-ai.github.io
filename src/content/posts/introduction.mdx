---
title: "Welcome to My AI Journey: From Physics to Machine Learning"
slug: "introduction"
date: 2025-05-30
excerpt: "An introduction to my background in physics, transition to AI research, and passion for interpretability and alignment. Join me as I document my learning journey and share insights."
types: ["blog"]
category: "Reflection"
tags: ["Introduction", "AI", "Physics", "Career", "Personal"]
status: "published"
display:
  showToc: true
  showRelated: true
  layout: "default"
  accent: "blue"
---

## Hello, and Welcome!

I'm Luis Miguel Montoya, and I'm excited to share my journey into the fascinating world of artificial intelligence with you. This blog serves as both a learning log and a way to connect with others who share my passion for understanding how AI systems work and ensuring they align with human values.

## My Background: A Physicist's Path to AI

My journey began in physics, where I developed a deep appreciation for mathematical rigor and systematic problem-solving. The transition from studying the fundamental laws of nature to understanding artificial minds might seem like a leap, but there's more overlap than you might think.

Physics taught me to:
- **Think in systems** - Understanding how complex behaviors emerge from simple rules
- **Embrace uncertainty** - Working with probabilistic models and statistical mechanics
- **Value interpretability** - Always asking "why does this work?" rather than just "does it work?"
- **Pursue first principles** - Breaking down complex problems into fundamental components

These skills have proven invaluable as I've delved into machine learning, where we're often trying to understand emergent behaviors in systems with millions or billions of parameters.

## Why AI Interpretability and Alignment?

As AI systems become more powerful and integrated into critical aspects of our lives, two questions become increasingly urgent:

### 1. **How do these systems actually work?** (Interpretability)
Current large language models and neural networks are often described as "black boxes." While we can see their inputs and outputs, understanding their internal reasoning processes remains challenging. This isn't just an academic curiosity—it's essential for:

- Building trust in AI systems
- Debugging when things go wrong
- Ensuring fairness and removing bias
- Meeting regulatory requirements for explainable AI

### 2. **How do we ensure they do what we want them to do?** (Alignment)
As AI systems become more capable, ensuring they pursue goals aligned with human values becomes critical. This involves:

- Understanding objective specification and reward modeling
- Preventing harmful behaviors and unintended consequences
- Developing robust safety measures for increasingly autonomous systems
- Creating frameworks for human-AI collaboration

## What You'll Find Here

This blog will document my exploration of these topics through several types of content:

### **Learning Journey Posts**
I'll share my experiences diving into key papers, implementing algorithms from scratch, and working through challenging concepts. Think of these as "learning in public"—documenting both successes and struggles along the way.

### **Technical Deep Dives**
When I encounter particularly interesting concepts or techniques, I'll break them down in detail, often with code examples and visualizations to help make complex ideas more accessible.

### **Research Reviews**
The field moves quickly, and I'll regularly review and summarize important new papers, highlighting key insights and their potential implications.

### **Project Showcases**
As I build projects and tools related to interpretability and alignment, I'll share the process, challenges, and results here.

### **Reflection and Commentary**
Sometimes I'll step back to reflect on broader questions about the direction of AI research, the societal implications of our work, and the ethical considerations we must keep in mind.

## My Learning Philosophy

I believe in **learning by doing**. While reading papers and watching lectures is important, I find I truly understand concepts only when I implement them myself. You'll often see me:

- **Building things from scratch** to understand the underlying mechanisms
- **Creating visualizations** to make abstract concepts concrete
- **Documenting failures** as well as successes (they're often more instructive!)
- **Connecting concepts** across different areas of AI and beyond

## Current Focus Areas

Right now, I'm particularly interested in:

- **Mechanistic interpretability** - Understanding how neural networks implement algorithms
- **Activation patching and causal analysis** - Techniques for isolating the causal role of different model components
- **Constitutional AI and reward modeling** - Methods for training AI systems to be helpful, harmless, and honest
- **AI safety research** - Technical approaches to ensuring advanced AI systems remain aligned with human values

## Join the Conversation

This journey is more rewarding when shared. I'd love to hear from you if:

- You're also learning about AI interpretability or alignment
- You have questions about topics I've covered
- You spot errors or have suggestions for improvement
- You're working on related projects and want to collaborate

You can find me on [Twitter/X](https://twitter.com/lmmontoya_), [LinkedIn](https://linkedin.com/in/lmmontoya), or reach out via email through my [contact page](/contact).

## What's Next?

In upcoming posts, I'll be diving into:

1. **Understanding Transformer Architectures** - A deep dive into attention mechanisms and how they enable language understanding
2. **Implementing Activation Patching** - Building tools to understand causal relationships in neural networks
3. **Exploring RLHF** - How reinforcement learning from human feedback shapes AI behavior
4. **Constitutional AI Deep Dive** - Understanding Anthropic's approach to training helpful, harmless, and honest AI

## A Personal Note

Starting a blog like this feels a bit vulnerable—putting my learning process and half-formed thoughts out into the world. But I've found that some of the most valuable resources for my own learning have been others' honest accounts of their journeys, complete with confusion, mistakes, and gradual understanding.

If you're also early in your AI journey, I hope this blog helps you feel less alone in the process. If you're more experienced, I'd be grateful for your insights and corrections.

The field of AI safety and interpretability is too important for any of us to tackle alone. Together, we can work toward AI systems that are not just powerful, but also understandable, safe, and aligned with human flourishing.

Thank you for joining me on this journey. Let's learn together.

---

*This introduction post marks the beginning of what I hope will be a long and fruitful exploration. If you'd like to stay updated on new posts, consider bookmarking this site or following me on social media. The adventure in AI interpretability and alignment starts now!*
